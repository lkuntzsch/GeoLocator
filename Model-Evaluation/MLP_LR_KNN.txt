Epoch 01 | Train: loss=1.5235, acc=0.5140 | Val:   loss=1.2309, acc=0.5990
Epoch 02 | Train: loss=1.3367, acc=0.5674 | Val:   loss=1.1479, acc=0.6239
Epoch 03 | Train: loss=1.2764, acc=0.5860 | Val:   loss=1.1064, acc=0.6356
Epoch 04 | Train: loss=1.2439, acc=0.5960 | Val:   loss=1.0795, acc=0.6440
Epoch 05 | Train: loss=1.2213, acc=0.6024 | Val:   loss=1.0598, acc=0.6526
Epoch 06 | Train: loss=1.2067, acc=0.6070 | Val:   loss=1.0465, acc=0.6565
Epoch 07 | Train: loss=1.1959, acc=0.6101 | Val:   loss=1.0371, acc=0.6577
Epoch 08 | Train: loss=1.1865, acc=0.6127 | Val:   loss=1.0292, acc=0.6631
Epoch 09 | Train: loss=1.1788, acc=0.6152 | Val:   loss=1.0183, acc=0.6643
Epoch 10 | Train: loss=1.1733, acc=0.6169 | Val:   loss=1.0161, acc=0.6650
Epoch 11 | Train: loss=1.1690, acc=0.6183 | Val:   loss=1.0130, acc=0.6667
Epoch 12 | Train: loss=1.1640, acc=0.6202 | Val:   loss=1.0113, acc=0.6695
Epoch 13 | Train: loss=1.1593, acc=0.6213 | Val:   loss=1.0053, acc=0.6689
Epoch 14 | Train: loss=1.1557, acc=0.6229 | Val:   loss=0.9997, acc=0.6706
Epoch 15 | Train: loss=1.1520, acc=0.6236 | Val:   loss=1.0002, acc=0.6706
Epoch 16 | Train: loss=1.1495, acc=0.6244 | Val:   loss=0.9962, acc=0.6725
Epoch 17 | Train: loss=1.1455, acc=0.6254 | Val:   loss=0.9891, acc=0.6736
Epoch 18 | Train: loss=1.1433, acc=0.6258 | Val:   loss=0.9894, acc=0.6749
Epoch 19 | Train: loss=1.1415, acc=0.6266 | Val:   loss=0.9833, acc=0.6753
Epoch 20 | Train: loss=1.1378, acc=0.6279 | Val:   loss=0.9844, acc=0.6757
Epoch 21 | Train: loss=1.1365, acc=0.6285 | Val:   loss=0.9827, acc=0.6751
Epoch 22 | Train: loss=1.1334, acc=0.6292 | Val:   loss=0.9770, acc=0.6766
Epoch 23 | Train: loss=1.1315, acc=0.6299 | Val:   loss=0.9752, acc=0.6770
Epoch 24 | Train: loss=1.1287, acc=0.6304 | Val:   loss=0.9765, acc=0.6790
Epoch 25 | Train: loss=1.1261, acc=0.6315 | Val:   loss=0.9745, acc=0.6776
Epoch 26 | Train: loss=1.1241, acc=0.6320 | Val:   loss=0.9700, acc=0.6783
Epoch 27 | Train: loss=1.1206, acc=0.6332 | Val:   loss=0.9658, acc=0.6830
Epoch 28 | Train: loss=1.1184, acc=0.6339 | Val:   loss=0.9641, acc=0.6819
Epoch 29 | Train: loss=1.1170, acc=0.6339 | Val:   loss=0.9638, acc=0.6828
Epoch 30 | Train: loss=1.1158, acc=0.6349 | Val:   loss=0.9637, acc=0.6815
Epoch 31 | Train: loss=1.1110, acc=0.6358 | Val:   loss=0.9599, acc=0.6829
Epoch 32 | Train: loss=1.1103, acc=0.6359 | Val:   loss=0.9556, acc=0.6853
Epoch 33 | Train: loss=1.1074, acc=0.6368 | Val:   loss=0.9570, acc=0.6830
Epoch 34 | Train: loss=1.1057, acc=0.6378 | Val:   loss=0.9561, acc=0.6854
Early stopping nach 2 Epochen ohne Verbesserung.

=== Testset: loss=0.9615, acc=0.6823, top-5=0.9326 ===

Classification Report:
              precision    recall  f1-score   support

          AT       0.56      0.51      0.53      9997
          BE       0.66      0.63      0.65      9992
          BG       0.81      0.83      0.82      9534
          CH       0.65      0.63      0.64      9999
          CZ       0.62      0.58      0.60      9997
          DE       0.55      0.46      0.50     20000
          DK       0.50      0.67      0.57     10000
          EE       0.78      0.76      0.77      9985
          ES       0.71      0.66      0.68     19796
          FI       0.69      0.85      0.76      9998
          FR       0.72      0.51      0.59     20000
          GB       0.80      0.76      0.78     20000
          GR       0.75      0.80      0.77      9998
          HR       0.63      0.52      0.57      9991
          HU       0.53      0.63      0.58      9996
          IE       0.69      0.87      0.77      9999
          IT       0.63      0.59      0.61     20000
          LT       0.83      0.76      0.79     10515
          LU       0.82      0.88      0.85      9974
          LV       0.56      0.75      0.64      8100
          MT       0.94      0.98      0.96      7625
          NL       0.59      0.77      0.67     10000
          NO       0.69      0.77      0.73      9977
          PL       0.65      0.59      0.62     20000
          PT       0.58      0.75      0.66     10448
          RO       0.82      0.80      0.81     11523
          RS       0.69      0.74      0.71      9998
          SE       0.81      0.67      0.73     20000
          SI       0.67      0.72      0.69      9990
          SK       0.62      0.65      0.63      9998

    accuracy                           0.68    367430
   macro avg       0.69      0.70      0.69    367430
weighted avg       0.69      0.68      0.68    367430


Confusion Matrix:
[[ 5083   142    45   773   390   634   127    46    84    77   129    44
     16   109   194    19   441    37   149    76     2    72   225   309
     69    96    74    92   272   171]
 [   62  6339    16   130   149   476   280    23    46    29   293   126
      6    62   117    97   102    53   286    60    32   611    43   194
     30    13    46    55    50   166]
 [   10     7  7889    46    56    16    10     8    55     8    13    14
    158   113    63    16    68    51    35    31    22     7    27    45
     45   158   318     6    71   168]
 [  618   167    83  6322   174   459    96    13    64    35    88    39
     13    70    45    52   337    45   252    35    14    79   255    55
     40    24    78    37   260   150]
 [  204   169    98   183  5772   415    93    43    83    45   135    76
      4    94   258    44   110   112   283   113     6    62   169   390
     52    77   106    58   202   541]
 [  727   554    25   362   494  9147  1405    92   203   232   594   575
     17   109   601   201   406    88    46   287     2  1401   164  1350
     97   117    99   386    73   146]
 [   46   197    12    35    41   543  6722    92    49    74   141   339
      7    10   113   152    64    41    26   157     8   465    99   191
     51     9    32   235     7    42]
 [    9    25    14    12    27    53   140  7604     5   204    14    16
      1    13    38    20    15   125    23  1121     2    37    76   134
     15    11    21   168    16    26]
 [   92    50    56    93    83   137   103     6 12981    48   438   198
    659   191   125   154  1263    16     9    41    73   116   107   169
   2227   110   105    55    22    69]
 [   20    17     2     8    11   100    62   128    21  8459     9    15
      3     2    16    16    12    24     9   152     0    43   307    86
     19     6     3   438     3     7]
 [  286   399    31   226   165   859   820    46  1063    62 10143   672
    160   192   467   495  1235    42    10   110    11   442   119   470
    821   118   143   217    54   122]
 [   29   159    14    21    42   306   687    16   124    28   376 15200
     19    10   142  1683   111     7     9    41     1   405   118   130
    138    36    27    94     6    21]
 [   14     4   193    24    14     1    18     7   341     4    24    12
   7965   177    50    21   389     6    10    10    78     4    28    31
    268    95   131     5    22    52]
 [  111    46   168   134   122    69    20    25   169    10    71    18
    227  5218   282    29   301    45   116    32   107    33    60   177
     95    56   379    22  1623   226]
 [   88    95    49    41   193   221   123    51    78    21   166    72
     40   213  6292    58   200    81    38   107     1   104    55   405
     46   176   253    29   218   482]
 [    3    36     6    15    17    45   108     6    26    18    78   603
      9     7    27  8678    36     2    18    11    11    58    49    25
     77    11     3     9     1     6]
 [  566   150    77   474    84   278   192    20  1421    40   482   138
    650   321   427   162 11885    13     9    61    48   238   158   314
    894   198   334    71   131   164]
 [   19    58    44    37   121   104    80   169    25   106    18     9
      3    18   126    17    25  8021    87   695     6    58    69   249
     16    26    41    79    29   160]
 [   29   150    14    95   110   154    47     2    10     3    37    31
      5    18    34    28    17    40  8814    21     7    45    26    23
     10     3    20     6    58   117]
 [   14    42    21     4    62    57   110   541     7   178    16    16
      2     7    73    13    11   285    51  6077     5    36    67   167
     12     6    29   114    11    66]
 [    0     4     6     2     1     0     1     0    18     0     4     0
     27    24     0     7    18     0     6     0  7481     0     1     1
     14     0     0     0     4     6]
 [   18   275     1    32    29   473   381    17    35    33    83   186
      2    13    96    59    72    24     7    55     0  7716    16   252
     18     7    14    70     1    15]
 [   76    44    28   129    74    89   113    90    33   357    35    65
     11    27    13    56    43    40    61   100    11    20  7717    38
     51    18    27   531    39    41]
 [  274   190    70    38   384  1132   537   245   194   304   268   188
     30   157   910   113   346   203    16   695     0   603   125 11743
    180   246   201   286    48   274]
 [   19    21    16    18    24    41    66     4   898    19   142    66
    289    50    45   122   433     7     3    15    17    30    62    59
   7843    44    57    16     5    17]
 [   59     9   244    18    67    67    27     9   101    29    58    41
     78    65   384    39   185    12     0    22     1    31    51   178
     77  9238   278    18    25   112]
 [   46    34   235    54    78    54    30    29    69    10    35    13
     91   176   294    22   208    67    46    44    12    20    51   149
     63   224  7377    13   136   318]
 [   68    89    12    50    42   481   945   368    69  1826   167   142
     11    35    76    66    69    60     4   508     0   311   732   304
    103    19    23 13380     6    34]
 [  271    44    97   218   146    82    20    19    34    13    45     5
     17   616    66    26   234    43   140    44    24     7    86    84
     32    20   193    19  7152   193]
 [  186   114   115   118   412   136    69    20    92    23    75    31
     40   113   481    33   152   114   176    96    10    29    90   226
     36    97   241    34   183  6456]]

        epochs=100,
        batch_size=128,
        lr=5e-4,
        weight_decay=1e-4,
        hidden_dims=[512, 256],
        dropout=0.3,
        patience=2,

        Tuned version:

        epochs=100,
        batch_size=128,
        lr=0.0005805846519752405,          # von Optuna Trial 5
        weight_decay=0.0003084652555757216, # von Optuna Trial 5
        hidden_dims=[1024, 512],           # von Optuna Trial 5
        dropout=0.21911440122299433,       # von Optuna Trial 5
        patience=10,
        lr_factor=0.5,
        lr_patience=5,






PS C:\Users\master\Desktop\Bv-Dateien> python .\train_mlp.py

=== Logistic Regression ===
Accuracy: 0.5454
Top-5 Accuracy: 0.8720
Classification Report:
              precision    recall  f1-score   support

          AT       0.36      0.38      0.37      9997
          BE       0.45      0.46      0.46      9992
          BG       0.56      0.61      0.59      9534
          CH       0.44      0.43      0.43      9999
          CZ       0.36      0.31      0.33      9997
          DE       0.46      0.35      0.40     20000
          DK       0.40      0.53      0.46     10000
          EE       0.58      0.62      0.60      9985
          ES       0.62      0.54      0.57     19796
          FI       0.60      0.76      0.67      9998
          FR       0.54      0.43      0.48     20000
          GB       0.74      0.71      0.72     20000
          GR       0.60      0.70      0.64      9998
          HR       0.41      0.34      0.37      9991
          HU       0.38      0.46      0.41      9996
          IE       0.65      0.79      0.71      9999
          IT       0.55      0.47      0.51     20000
          LT       0.62      0.59      0.60     10515
          LU       0.64      0.73      0.69      9974
          LV       0.45      0.59      0.51      8100
          MT       0.92      0.97      0.95      7625
          NL       0.52      0.69      0.59     10000
          NO       0.60      0.63      0.61      9977
          PL       0.50      0.42      0.46     20000
          PT       0.48      0.63      0.54     10448
          RO       0.65      0.70      0.68     11523
          RS       0.46      0.47      0.47      9998
          SE       0.72      0.56      0.63     20000
          SI       0.51      0.54      0.53      9990
          SK       0.39      0.37      0.38      9998

    accuracy                           0.55    367430
   macro avg       0.54      0.56      0.55    367430
weighted avg       0.55      0.55      0.54    367430

Confusion Matrix:
[[ 3812   176    90   970   402   568   151    65   119    96   253    49
     38   205   225    27   466    92   259    67     2    90   230   424
     79   177   147   140   355   223]
 [  118  4596    73   230   187   529   315    78    64    34   403   196
     12   113   200   137   134   156   561    74    33   742    82   297
     53    31   119    85    83   257]
 [   37    42  5795   108   118    39    33    35    99    14    46    27
    338   252   202    19    67   113    95    62    25     7    77    95
    116   517   565    13   238   340]
 [ 1010   279   164  4273   304   406    95    43    83    53   152    50
     24   172    62    54   410   147   444    30    15    76   262    75
     57    30   210    64   631   324]
 [  322   302   268   376  3062   483   154   151   122    62   255    91
     26   164   384    51   139   323   545   187     4    72   179   560
     72   210   239    79   447   668]
 [ 1169   688    75   408   643  7000  1488   168   181   250   871   582
     40   160   718   162   416   132   103   340     0  1511   157  1588
    139   181   149   424    65   192]
 [   73   360    35    45    71   628  5349   226    59    90   308   486
      9    34   134   175    67    62    54   214    14   606   112   289
     58    32    44   302    18    46]
 [   24    62    37    22    72    73   228  6224     8   417    34    36
      3    33    98    24    18   335    56  1211     9    57   105   245
     23    15    80   307    45    84]
 [  222   107   178   146   115   137   142    12 10652    49   771   272
   1058   278   162   190  1481    28    29    64    99    90   126   168
   2592   267   185    52    44    80]
 [   47    35    10    20    26    91    72   280    19  7601    19    12
      2    13    25    31    17    47    24   269     0    46   415   135
     33    20    14   653    12    10]
 [  414   666    77   357   150   886   939    90  1107    66  8558   840
    293   226   598   516  1111    59    45   109    16   487   118   557
    893   181   180   222    83   156]
 [   32   279    26    38    59   348   824    47   175    38   532 14171
     38    24   171  1891    94    24    37    51     2   371   145   171
    157    79    35   113     7    21]
 [   18    11   360    38    18     5    26     9   485     7    80    17
   6951   340    61    28   371     9    24     6   111     7    33    39
    508   167   169     9    37    54]
 [  170   120   348   321   215   113    40    48   263    21   146    18
    497  3428   322    29   428    83   250    35    98    56    78   259
    170   159   556    12  1342   366]
 [  110   166   181   104   269   294   184   116   118    34   296   101
     68   234  4562    74   240   145   123   105     1   145    38   634
     81   395   384    51   247   496]
 [    8    43    15    22    15    51   149    15    36    22   170   984
     20    19    36  7944    38     9    55    15    17    45    57    49
     96    21     8    22     8    10]
 [  923   236   216   413   103   265   161    26  1568    60   873   155
   1051   459   521   164  9383    40    21    67    68   264   147   400
   1138   265   437    78   236   262]
 [   34   169   152   106   339   102   103   396    39   145    55    14
      8    61   158    16    37  6201   289   931     3    81    97   318
     16    49   137   122    61   276]
 [   38   377   112   271   261   124    72    38     8    13    76    57
      5    81    60    63    20   161  7325    42    12    48    70    28
     15     3   101     3   246   244]
 [   20    65    54    21   116    86   171   858     7   276    27    16
      7    14    69    20    19   508    97  4805     1    39    79   287
     30    30    41   204    34    99]
 [    0     9    24     7     1     0     1     3    30     0     3     1
     32    37     2    10    22     2     5     0  7394     0     6     0
     26     0     2     0     3     5]
 [   45   358     7    30    50   521   616    33    48    36   171   234
     10    13   128    58    75    41    21    53     0  6885    13   349
     26    20    26   107     3    23]
 [  178    72    59   221    91    90   147   269    49   582    62    91
     21    42    20    93    68   117   100   152    12    47  6282    66
 [  178    72    59   221    91    90   147   269    49   582    62    91
     21    42    20    93    68   117   100   152    12    47  6282    66
     21    42    20    93    68   117   100   152    12    47  6282    66
     75    44    43   742    84    58]
 [  506   341   127   127   392  1431   672   432   238   358   603   258
     78   163  1180   112   443   387    28   764     2   829   179  8403
    222   509   352   379    51   434]
 [   37    34    67    44    32    28    81    27  1223    37   299    95
    490   114    78   157   532    21    11    34    26    37    71    80
   6542    93    86    35    13    24]
 [  193    27   379    42    86    69    22    29   150    39   106    48
    207   117   549    39   189    39     6    21     3    65    50   245
    120  8077   387    25    31   163]
 [  106    91   666   132   141    66    76    50    95    18   108    26
    177   459   460    13   327   233   139    67    11    38    88   223
    105   453  4728    21   302   579]
 [  176   140    16    89   116   469  1013   755    70  2230   311   192
     12    42   108    65    83   108    13   609     0   365  1007   422
    119    41    61 11296    23    49]
 [  526    96   279   505   286   110    28    51    59    22    73    17
     42   807    99    29   277    70   273    50    21    14   143    91
     45    47   263    14  5383   270]
 [  282   231   382   250   670   177   112    83    92    27   174    59
     50   267   607    31   185   335   379   155    13    58   111   358
     57   270   510    34   328  3711]]

