# Embeddings mit CLIP
*Info Sammlung zum nachlesen*

###**CLIP wird nicht zum Labeln der Bilder verwendet, sondern zur Repräsentation.**

---

## **Was macht CLIP?**

CLIP nimmt ein Bild (oder einen Text) und gibt einen Embedding-Vektor zurück – ein 512-dimensionaler Zahlenvektor. Dieser Vektor enthält semantische Informationen darüber, was im Bild ist.
CLIP gibt **kein Label** wie „Italien“ oder „Wald“

Sagt etwas aus wie: „Das Bild zeigt etwas, das Pflanzen, Hügel und ein rot-gelbes Dach enthält.“

---

##  **Woher kommen die Labels?**

Die Labels (z. B. **Länder oder Regionen**) stammen von externen Datenquellen (zB GeoYFCC: Bilder+Geokoordinaten)
In Trainingsdaten hat jedes Bild: 
    
    - das tatsächliche Bild (`.jpg`)
        
    - das zugehörige Label (`"Italien"`)
        

---

## **Funktionsweise**

1. 100.000 Bilder mit bekannten Labels sind vorhanden
    
2. Jedes Bild durch CLIP -> je ein Embedding (512 Werte)
    
3. Traineiren eines k-NN-Classifiers / Logistic Regression (und MLP), der auf diesen Embeddings lernt und mit gewisser Wahrscheinlichkeit ein Land vorhersagt.
    
4. Wenn Nutzer ein neues Bild hochlädt:
    
    - Erzeugen des Embeddings
        
    - Anwendung Modell - welches Embedding passt am besten?
